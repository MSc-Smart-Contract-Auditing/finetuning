{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "MODEL_ALIAS = 'llama3.1-8b'\n",
            "MODEL_DIR = Path(MODEL_ALIAS)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Run evaluations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from openai import OpenAI\n",
            "from config import OPENAI_API_KEY\n",
            "client = OpenAI(\n",
            "    api_key=OPENAI_API_KEY\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pydantic import BaseModel\n",
            "\n",
            "class Response(BaseModel):\n",
            "    message: str\n",
            "    total_tokens: int\n",
            "    obj: object\n",
            "\n",
            "    def entries(self):\n",
            "        return list(map(\n",
            "            lambda c: 'PASS' if c == '1' else 'FAIL',\n",
            "            self.message,\n",
            "        ))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open('prompts/evaluation.txt', 'r') as f:\n",
            "    SYSTEM_PROMPT = f.read()\n",
            "\n",
            "def evaluate_output(output, real) -> Response:\n",
            "    response = client.chat.completions.create(\n",
            "        model=\"gpt-4o-mini\",\n",
            "        messages=[\n",
            "            {\n",
            "                \"role\": \"system\",\n",
            "                \"content\": [{\n",
            "                    \"type\": \"text\",\n",
            "                    \"text\": SYSTEM_PROMPT\n",
            "                }]\n",
            "            },\n",
            "            {\n",
            "                \"role\": \"user\",\n",
            "                \"content\": [{\n",
            "                    \"text\": f\"Student audit:\\n{output}\\n\\nGround truth:{real}\",\n",
            "                    \"type\": \"text\"\n",
            "                }]\n",
            "            },\n",
            "        ],\n",
            "        temperature=0,\n",
            "        max_tokens=256,\n",
            "        top_p=1,\n",
            "        frequency_penalty=0,\n",
            "        presence_penalty=0,\n",
            "        response_format={\n",
            "            \"type\": \"text\"\n",
            "        }\n",
            "    )\n",
            "\n",
            "    return Response(**{\n",
            "        'message': response.choices[0].message.content,\n",
            "        'total_tokens': response.usage.total_tokens,\n",
            "        'obj': response\n",
            "    })"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "\n",
            "model_alias = 'llama3.1-8b'\n",
            "\n",
            "MODEL_DIR = Path(model_alias)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "data = pd.read_csv(MODEL_DIR/f'{MODEL_ALIAS}-outputs.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def prepare_data(data):\n",
            "    data.loc[data[\"real\"].isnull(), 'real'] = \"There is no vulnearbility\"\n",
            "    data.apply(lambda col: col.replace('\\\\n', '\\n'))\n",
            "    return data\n",
            "\n",
            "data = prepare_data(data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "PRICE_PER_1M_INPUT_TOKENS = 0.150\n",
            "def calculate_cost_milli_dollars(total_tokens):\n",
            "    cost_dollars = (total_tokens / 1_000_000) * PRICE_PER_1M_INPUT_TOKENS\n",
            "    cost_milli_dollars = cost_dollars * 100\n",
            "    return cost_milli_dollars"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "import csv\n",
            "\n",
            "total_tokens = 0\n",
            "\n",
            "with open(MODEL_DIR / 'evaluation.csv', 'w') as f:\n",
            "    w = csv.writer(f)\n",
            "    w.writerow(['id', 'cr0', 'cr1', 'cr2', 'cr3'])\n",
            "    with tqdm(data.iterrows(), total=len(data), desc=\"Processing\", unit=\"row\") as progress_bar:\n",
            "        for idx, row in progress_bar:\n",
            "            vuln_out = row.output != 'There is no vulnearbility'\n",
            "            vuln_real = row.real != 'There is no vulnearbility'\n",
            "\n",
            "            if not vuln_out and not vuln_real:\n",
            "                w.writerow([idx, 'PASS', 'PASS', 'PASS', 'PASS'])\n",
            "                continue\n",
            "\n",
            "            if vuln_out != vuln_real:\n",
            "                w.writerow([idx, 'FAIL', 'FAIL', 'FAIL', 'FAIL'])\n",
            "                continue\n",
            "\n",
            "            r = evaluate_output(row.output, row.real)\n",
            "            w.writerow([idx, 'PASS'] + r.entries())\n",
            "            total_tokens += r.total_tokens\n",
            "            progress_bar.set_postfix({\n",
            "                'Price (0.01$)': calculate_cost_milli_dollars(total_tokens),\n",
            "                'tokens': total_tokens\n",
            "            })"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Plot evaluations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "\n",
            "data = pd.read_csv(MODEL_DIR / 'evaluation.csv').drop(columns=['id'])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "from sklearn.metrics import accuracy_score, f1_score\n",
            "\n",
            "# Calculate accuracy and F1 score for each criterion\n",
            "def compute_metrics(data):\n",
            "\n",
            "    data = data == 'PASS'\n",
            "    criteria = list(data.columns)\n",
            "    overall = data.all(axis=1)\n",
            "    # print(overall[overall])\n",
            "    ideal = [1] * len(data[criteria[0]])\n",
            "\n",
            "    acc = [\n",
            "        accuracy_score(data[criterion], ideal) for criterion in criteria\n",
            "    ]\n",
            "    acc.append(accuracy_score(overall, ideal))\n",
            "\n",
            "    f1 = [\n",
            "        f1_score(data[criterion], ideal) for criterion in criteria\n",
            "    ]\n",
            "    f1.append(f1_score(overall, ideal))\n",
            "\n",
            "    return acc, f1\n",
            "\n",
            "# Compute metrics for individual criteria\n",
            "labels = ['Criterion 1', 'Criterion 2', 'Criterion 3', 'Criterion 4', 'Overall']\n",
            "colors = ['turquoise', 'limegreen', 'darkorange', 'orangered', 'violet']\n",
            "accuracy, f1_scores = compute_metrics(data)\n",
            "# Create plots\n",
            "fig, ax = plt.subplots(2, 1, figsize=(6, 7), sharex=True)\n",
            "\n",
            "\n",
            "# Plot Accuracy\n",
            "bars = ax[0].bar(labels, accuracy, color=colors)\n",
            "ax[0].set_ylabel('Accuracy', fontsize=14)\n",
            "ax[0].set_ylim([0, 1])\n",
            "ax[0].set_title(f'{MODEL_ALIAS} Performance Metrics', fontsize=16)\n",
            "ax[0].grid(axis='y')\n",
            "fontdict = {'size': 11, 'weight': 'bold'}\n",
            "for idx, bar in enumerate(bars):\n",
            "    ax[0].text(bar.get_x() + bar.get_width() / 2.0 + 0.2, accuracy[idx]+0.02, f'{accuracy[idx]:.3f}', color=colors[idx], va='center', ha='right', fontdict=fontdict)\n",
            "\n",
            "# Plot F1 Score\n",
            "bars = ax[1].bar(labels, f1_scores, color=colors)\n",
            "ax[1].set_ylabel('F1 Score', fontsize=14)\n",
            "ax[1].set_ylim([0, 1])\n",
            "ax[1].grid(axis='y')\n",
            "\n",
            "ax[1].set_yticklabels([f'{0.2 * n:.1f}' for n in range(0, 5)])\n",
            "ax[0].set_yticklabels(['0.0\\n1.0', *[f'{0.2 * n:.1f}' for n in range(1, 6)]])\n",
            "\n",
            "for idx, bar in enumerate(bars):\n",
            "    ax[1].text(bar.get_x() + bar.get_width() / 2.0 + 0.2, f1_scores[idx]+0.02, f'{f1_scores[idx]:.3f}', color=colors[idx], va='center', ha='right', fontdict=fontdict)\n",
            "\n",
            "ax[0].axvline(x=0.3 + bars[0].get_width() * 4, color='black', linestyle='--', linewidth=1.5)\n",
            "ax[1].axvline(x=0.3 + bars[0].get_width() * 4, color='black', linestyle='--', linewidth=1.5)\n",
            "\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.subplots_adjust(hspace=0.00)\n",
            "plt.savefig(MODEL_DIR / 'metrics.pdf', format='pdf', bbox_inches='tight', pad_inches=0)\n",
            "plt.savefig(MODEL_DIR / 'metrics.png', format='png', bbox_inches='tight', pad_inches=0)\n",
            "plt.show()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "rl",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.7"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
