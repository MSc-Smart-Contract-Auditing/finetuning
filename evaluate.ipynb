{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'unsloth/Meta-Llama-3.1-8B-Instruct-bnb-4bit'\n",
    "model_alias = 'llama3.1-8b'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "import re\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path('/vol/bitbucket/kza23/finetuning')\n",
    "WORK_DIR = BASE_DIR / model_alias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 20 # Choose any! We auto support RoPE Scaling internally!\n",
    "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
    "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = model_name,\n",
    "    max_seq_length = max_seq_length,\n",
    "    dtype = dtype,\n",
    "    load_in_4bit = load_in_4bit,\n",
    ")\n",
    "\n",
    "FastLanguageModel.for_inference(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.read_csv(WORK_DIR/f\"{model_alias}-outputs.csv\")\n",
    "results.loc[results[\"real\"].isnull(), 'real'] = \"There is no vulnearbility\"\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(BASE_DIR/'prompts/evaluation.txt', mode='r') as f:\n",
    "    eval_prompt = f.read()\n",
    "\n",
    "split_token = eval_prompt.split('\\n')[-2]\n",
    "\n",
    "criteria = []\n",
    "for idx in range(1, 4):\n",
    "    with open(BASE_DIR/f'prompts/criterion{idx}.txt', mode='r') as f:\n",
    "        criteria.append(f.read())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(eval_prompt)\n",
    "print(split_token)\n",
    "for cirterion in criteria:\n",
    "    print(cirterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_criteria(query):\n",
    "    ret_reasoning = ''\n",
    "    verdicts = []\n",
    "    for idx, criterion in enumerate(criteria):\n",
    "        format_input = query.format(criterion).replace('{{', '{').replace('}}', '}')\n",
    "        inputs = tokenizer(format_input, return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
    "        output_tokens = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            use_cache = True\n",
    "        )\n",
    "\n",
    "        decoded_output = tokenizer.decode(\n",
    "            output_tokens[0],\n",
    "            skip_special_tokens=True,\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "        )\n",
    "\n",
    "        result = decoded_output.split(f\"{split_token}\\n\")[1].strip()\n",
    "        parts = result.split('Verdict: ')\n",
    "        if len(parts) == 2:\n",
    "            reasoning, verdict = parts\n",
    "        else:\n",
    "            verdict = \"N/A\"\n",
    "            reasoning = parts[0]\n",
    "\n",
    "        ret_reasoning += f'Criterion {idx+1}:\\n{reasoning.strip()}\\n'\n",
    "        verdicts.append(verdict.strip())\n",
    "    return verdicts, ret_reasoning\n",
    "\n",
    "def run_query(data, query, show_tqdm=True):\n",
    "    queries = data.apply(lambda row: query.format(\n",
    "        \"{}\",\n",
    "        row['real'].replace('\\\\n', '\\n').replace('{', '{{').replace('}', '}}'),\n",
    "        row['output'].replace('\\\\n', '\\n').replace('{', '{{').replace('}', '}}'),\n",
    "    ), axis=1)\n",
    "\n",
    "    if show_tqdm:\n",
    "        iterator = tqdm(enumerate(queries), total=len(queries))\n",
    "    else:\n",
    "        iterator = enumerate(queries)\n",
    "\n",
    "    for idx, query in iterator:\n",
    "        real_contains_vuln = data.iloc[idx][\"real\"] != \"There is no vulnearbility\"\n",
    "        output_contains_vuln = data.iloc[idx][\"output\"] != \"There is no vulnearbility\"\n",
    "\n",
    "        if real_contains_vuln != output_contains_vuln:\n",
    "            yield [\"FAIL\", \"FAIL\", \"FAIL\", \"FAIL\", \"Criterion 0: FAIL\\\\n One of the descriptions does not contain a vulnerability.\"]\n",
    "            continue\n",
    "\n",
    "        if not real_contains_vuln and not output_contains_vuln:\n",
    "            yield [\"PASS\", \"PASS\", \"PASS\", \"PASS\", \"No vulnerabilities to compare.\"]\n",
    "            continue\n",
    "\n",
    "\n",
    "        verdicts, reasoning = run_criteria(query)\n",
    "        verdicts = [\"PASS\"] + verdicts\n",
    "\n",
    "        yield verdicts + [reasoning.replace(\"\\n\", \"\\\\n\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(WORK_DIR/f\"{model_alias}-outputs.csv\", mode='w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow([\"cr0\", \"cr1\", \"cr2\", \"cr3\", \"Reasoning\"])\n",
    "    for row in run_query(results, eval_prompt):\n",
    "        print(row)\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
