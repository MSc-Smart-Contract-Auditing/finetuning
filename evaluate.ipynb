{
   "cells": [
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pathlib import Path\n",
            "MODEL_ALIAS = 'llama3.1-8b-4bit'\n",
            "MODEL_DIR = Path('runs') / MODEL_ALIAS"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Evalutations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from openai import OpenAI\n",
            "from config import OPENAI_API_KEY\n",
            "client = OpenAI(\n",
            "    api_key=OPENAI_API_KEY\n",
            ")"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from pydantic import BaseModel\n",
            "\n",
            "class Response(BaseModel):\n",
            "    message: str\n",
            "    total_tokens: int\n",
            "    obj: object\n",
            "\n",
            "    def entries(self):\n",
            "        return list(map(\n",
            "            lambda c: 'PASS' if c == '1' else 'FAIL',\n",
            "            self.message,\n",
            "        ))"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def prompt(messages) -> Response:\n",
            "    response = client.chat.completions.create(\n",
            "        model=\"gpt-4o-mini\",\n",
            "        messages=messages,\n",
            "        temperature=0,\n",
            "        max_tokens=256,\n",
            "        top_p=1,\n",
            "        frequency_penalty=0,\n",
            "        presence_penalty=0,\n",
            "        response_format={\n",
            "            \"type\": \"text\"\n",
            "        }\n",
            "    )\n",
            "\n",
            "    return Response(**{\n",
            "        'message': response.choices[0].message.content,\n",
            "        'total_tokens': response.usage.total_tokens,\n",
            "        'obj': response\n",
            "    })"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "PRICE_PER_1M_INPUT_TOKENS = 0.150\n",
            "def calculate_cost_milli_dollars(total_tokens):\n",
            "    cost_dollars = (total_tokens / 1_000_000) * PRICE_PER_1M_INPUT_TOKENS\n",
            "    cost_milli_dollars = cost_dollars * 100\n",
            "    return cost_milli_dollars"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Run description evaluations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open('evaluation-prompts/description.txt', 'r') as f:\n",
            "    SYSTEM_PROMPT_DESC = f.read()\n",
            "\n",
            "def evaluate_descriptions(output, real) -> Response:\n",
            "    messages = [\n",
            "        {\n",
            "            \"role\": \"system\",\n",
            "            \"content\": [{\n",
            "                \"type\": \"text\",\n",
            "                \"text\": SYSTEM_PROMPT_DESC\n",
            "            }]\n",
            "        },\n",
            "        {\n",
            "            \"role\": \"user\",\n",
            "            \"content\": [{\n",
            "                \"text\": f\"Student audit:\\n{output}\\n\\nGround truth:{real}\",\n",
            "                \"type\": \"text\"\n",
            "            }]\n",
            "        },\n",
            "    ]\n",
            "    return prompt(messages)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "data = pd.read_csv(MODEL_DIR/'descriptions.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def prepare_data(data):\n",
            "    data.loc[data[\"real\"].isnull(), 'real'] = \"There is no vulnerability\"\n",
            "    data.loc[data[\"output\"].isnull(), 'output'] = \"\"\n",
            "    data.apply(lambda col: col.replace('\\\\n', '\\n'))\n",
            "    return data\n",
            "\n",
            "data = prepare_data(data)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "import csv\n",
            "\n",
            "total_tokens = 0\n",
            "\n",
            "with open(MODEL_DIR / 'descriptions-eval.csv', 'w') as f:\n",
            "    w = csv.writer(f)\n",
            "    w.writerow(['id', 'cr0', 'cr1', 'cr2', 'cr3'])\n",
            "    with tqdm(data.iterrows(), total=len(data), desc=\"Processing\", unit=\"row\") as progress_bar:\n",
            "        for idx, row in progress_bar:\n",
            "            vuln_out = not row.output.startswith('There is no vulnerability')\n",
            "            vuln_real = not row.real.startswith('There is no vulnerability')\n",
            "\n",
            "            if not vuln_out and not vuln_real:\n",
            "                w.writerow([idx, 'PASS', 'PASS', 'PASS', 'PASS'])\n",
            "                continue\n",
            "\n",
            "            if vuln_out != vuln_real:\n",
            "                w.writerow([idx, 'FAIL', 'FAIL', 'FAIL', 'FAIL'])\n",
            "                continue\n",
            "\n",
            "            r = evaluate_descriptions(row.output, row.real)\n",
            "            w.writerow([idx, 'PASS'] + r.entries())\n",
            "            total_tokens += r.total_tokens\n",
            "            progress_bar.set_postfix({\n",
            "                'Price (0.01$)': calculate_cost_milli_dollars(total_tokens),\n",
            "                'tokens': total_tokens\n",
            "            })"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Run Recommendation evaluations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "with open('evaluation-prompts/recommendation.txt', 'r') as f:\n",
            "    SYSTEM_PROMPT_RECOMMENDATIONS = f.read()"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "def evaluate_recommendations(output, real):\n",
            "    messages = [\n",
            "        {\n",
            "            \"role\": \"system\",\n",
            "            \"content\": [{\n",
            "                \"type\": \"text\",\n",
            "                \"text\": SYSTEM_PROMPT_RECOMMENDATIONS\n",
            "            }]\n",
            "        },\n",
            "        {\n",
            "            \"role\": \"user\",\n",
            "            \"content\": [{\n",
            "                \"text\": f\"Output:\\n{output}\\n\\nReal:{real}\",\n",
            "                \"type\": \"text\"\n",
            "            }]\n",
            "        },\n",
            "    ]\n",
            "\n",
            "    return prompt(messages)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "data = pd.read_csv(MODEL_DIR/'recommendations.csv')"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from tqdm import tqdm\n",
            "import csv\n",
            "\n",
            "total_tokens = 0\n",
            "\n",
            "with open(MODEL_DIR / 'recommendations-eval.csv', 'w') as f:\n",
            "    w = csv.writer(f)\n",
            "    w.writerow(['id', 'cr'])\n",
            "    with tqdm(data.iterrows(), total=len(data), desc=\"Processing\", unit=\"row\") as progress_bar:\n",
            "        for idx, row in progress_bar:\n",
            "            r = evaluate_recommendations(row.output, row.real)\n",
            "            w.writerow([idx] + r.entries())\n",
            "            total_tokens += r.total_tokens\n",
            "            progress_bar.set_postfix({\n",
            "                'Price (0.01$)': calculate_cost_milli_dollars(total_tokens),\n",
            "                'tokens': total_tokens\n",
            "            })"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Plot evaluations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "from sklearn.metrics import accuracy_score, f1_score\n",
            "from pydantic import BaseModel\n",
            "\n",
            "class Statistic(BaseModel):\n",
            "    name: str\n",
            "    accuracy: float\n",
            "    f1: float\n",
            "\n",
            "    def __str__(self):\n",
            "        return f'acc={self.accuracy:.3f} f1={self.f1:.3f} <{self.name}>'\n",
            "\n",
            "    def __repr__(self):\n",
            "        return str(self)\n",
            "\n",
            "# Calculate accuracy and F1 score for each criterion\n",
            "def compute_overall_metrics(data, name) -> Statistic:\n",
            "    data = data == 'PASS'\n",
            "    overall = data.all(axis=1)\n",
            "    ideal = [1] * len(data[data.columns[0]])\n",
            "    return Statistic(\n",
            "        name=name,\n",
            "        accuracy=accuracy_score(overall, ideal),\n",
            "        f1=f1_score(overall, ideal, average='binary')\n",
            "    )"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Description evaluations"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import pandas as pd\n",
            "\n",
            "data = pd.read_csv(MODEL_DIR / 'descriptions-eval.csv').drop(columns=['id'])"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "from sklearn.metrics import accuracy_score, f1_score\n",
            "\n",
            "# Calculate accuracy and F1 score for each criterion\n",
            "def compute_metrics(data):\n",
            "\n",
            "    data = data == 'PASS'\n",
            "    criteria = list(data.columns)\n",
            "    overall = data.all(axis=1)\n",
            "    ideal = [1] * len(data[criteria[0]])\n",
            "\n",
            "    acc = [\n",
            "        accuracy_score(data[criterion], ideal) for criterion in criteria\n",
            "    ]\n",
            "    acc.append(accuracy_score(overall, ideal))\n",
            "\n",
            "    f1 = [\n",
            "        f1_score(data[criterion], ideal) for criterion in criteria\n",
            "    ]\n",
            "    f1.append(f1_score(overall, ideal))\n",
            "\n",
            "    return acc, f1\n",
            "\n",
            "# Compute metrics for individual criteria\n",
            "labels = ['CRIT 1', 'CRIT 2', 'CRIT 3', 'CRIT 4', 'OVERALL']\n",
            "colors = ['turquoise', 'limegreen', 'darkorange', 'orangered', 'violet']\n",
            "accuracy, f1_scores = compute_metrics(data)\n",
            "\n",
            "# Create plots\n",
            "fig, ax = plt.subplots(2, 1, figsize=(6, 7), sharex=True)\n",
            "\n",
            "# Plot Accuracy\n",
            "bars = ax[0].bar(labels, accuracy, color=colors)\n",
            "ax[0].set_ylabel('Accuracy', fontsize=14)\n",
            "ax[0].set_ylim([0, 1])\n",
            "ax[0].set_title(f'{MODEL_ALIAS} Performance Metrics', fontsize=16)\n",
            "ax[0].grid(axis='y')\n",
            "fontdict = {'size': 18, 'weight': 'bold'}\n",
            "for idx, bar in enumerate(bars):\n",
            "    ax[0].text(bar.get_x() + bar.get_width() / 2.0 + 0.4, accuracy[idx]+0.04, f'{accuracy[idx]:.3f}', color=colors[idx], va='center', ha='right', fontdict=fontdict)\n",
            "\n",
            "# Plot F1 Score\n",
            "bars = ax[1].bar(labels, f1_scores, color=colors)\n",
            "ax[1].set_ylabel('F1 Score (Binary)', fontsize=14)\n",
            "ax[1].set_ylim([0, 1])\n",
            "ax[1].grid(axis='y')\n",
            "\n",
            "ax[1].set_yticklabels([f'{0.2 * n:.1f}' for n in range(0, 5)])\n",
            "ax[0].set_yticklabels(['0.0\\n1.0', *[f'{0.2 * n:.1f}' for n in range(1, 6)]])\n",
            "\n",
            "for idx, bar in enumerate(bars):\n",
            "    ax[1].text(bar.get_x() + bar.get_width() / 2.0 + 0.4, f1_scores[idx]+0.04, f'{f1_scores[idx]:.3f}', color=colors[idx], va='center', ha='right', fontdict=fontdict)\n",
            "\n",
            "ax[0].axvline(x=0.3 + bars[0].get_width() * 4, color='black', linestyle='--', linewidth=1.5)\n",
            "ax[1].axvline(x=0.3 + bars[0].get_width() * 4, color='black', linestyle='--', linewidth=1.5)\n",
            "\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.subplots_adjust(hspace=0.00)\n",
            "plt.savefig(MODEL_DIR / f'{MODEL_ALIAS}-metrics.pdf', format='pdf', bbox_inches='tight', pad_inches=0)\n",
            "plt.savefig(MODEL_DIR / 'metrics.png', format='png', bbox_inches='tight', pad_inches=0)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Description Comparison"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "\n",
            "ls = !ls runs\n",
            "directories = [f'runs/{name[5:-6]}/descriptions-eval.csv' for name in ls]\n",
            "stats = []\n",
            "for dir in directories:\n",
            "    data = pd.read_csv(dir).drop(columns=['id'])\n",
            "    stats.append(compute_overall_metrics(data, dir.split('/')[1]))\n",
            "\n",
            "# Extracting model names, accuracies, and f1 scores\n",
            "model_names = [stat.name for stat in stats]\n",
            "accuracies = [stat.accuracy for stat in stats]\n",
            "f1_scores = [stat.f1 for stat in stats]\n",
            "\n",
            "# Creating the plots\n",
            "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # One row with two columns\n",
            "main_color = 'turquoise'\n",
            "highlight_color = 'limegreen'\n",
            "secondary_color = 'dodgerblue'\n",
            "highlight_model = 'deepseek-coder-6.7b-finetuned'\n",
            "secondary_model = 'gpt-4o-mini'\n",
            "\n",
            "colors = [main_color] * len(model_names)\n",
            "colors[model_names.index(highlight_model)] = highlight_color\n",
            "colors[model_names.index(secondary_model)] = secondary_color\n",
            "\n",
            "y_lim = min(max(max(accuracies), max(f1_scores)) + 0.1, 1)\n",
            "\n",
            "# Plot Accuracy\n",
            "\n",
            "bars_acc = ax[0].bar(model_names, accuracies, color=colors)\n",
            "ax[0].set_ylim([0, y_lim])\n",
            "ax[0].set_title('Accuracy', fontsize=16)\n",
            "ax[0].grid(axis='y')\n",
            "ax[0].set_xticks(np.arange(len(model_names)))\n",
            "ax[0].set_xticklabels(model_names, rotation=45, ha='right', fontsize=14)\n",
            "\n",
            "# Adding accuracy values above the bars\n",
            "for idx, bar in enumerate(bars_acc):\n",
            "    ax[0].text(bar.get_x() + bar.get_width() / 2.0, bar.get_height() + 0.01, f'{accuracies[idx]:.3f}', ha='center', va='bottom', fontsize=12, color='black')\n",
            "\n",
            "# Plot F1 Score\n",
            "bars_f1 = ax[1].bar(model_names, f1_scores, color=colors)\n",
            "ax[1].set_ylim([0, y_lim])\n",
            "ax[1].set_title('F1 Score (Binary)', fontsize=16)\n",
            "ax[1].grid(axis='y')\n",
            "ax[1].set_xticks(np.arange(len(model_names)))\n",
            "ax[1].set_xticklabels(model_names, rotation=45, ha='right', fontsize=14)\n",
            "\n",
            "# Adding F1 score values above the bars\n",
            "for idx, bar in enumerate(bars_f1):\n",
            "    ax[1].text(bar.get_x() + bar.get_width() / 2.0, bar.get_height() + 0.01, f'{f1_scores[idx]:.3f}', ha='center', va='bottom', fontsize=12, color='black')\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.savefig('all-descriptions.pdf', format='pdf', bbox_inches='tight', pad_inches=0)\n",
            "plt.savefig('all-descriptions.png', format='png', bbox_inches='tight', pad_inches=0)\n",
            "plt.show()"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Recommendation comparison"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": null,
         "metadata": {},
         "outputs": [],
         "source": [
            "import matplotlib.pyplot as plt\n",
            "import numpy as np\n",
            "import pandas as pd\n",
            "\n",
            "ls = !ls runs\n",
            "directories = [f'runs/{name[5:-6]}/recommendations-eval.csv' for name in ls]\n",
            "stats = []\n",
            "for dir in directories:\n",
            "    data = pd.read_csv(dir).drop(columns=['id'])\n",
            "    stats.append(compute_overall_metrics(data, dir.split('/')[1]))\n",
            "\n",
            "# Extracting model names, accuracies, and f1 scores\n",
            "model_names = [stat.name for stat in stats]\n",
            "accuracies = [stat.accuracy for stat in stats]\n",
            "f1_scores = [stat.f1 for stat in stats]\n",
            "\n",
            "# Creating the plots\n",
            "fig, ax = plt.subplots(1, 2, figsize=(12, 6))  # One row with two columns\n",
            "main_color = 'turquoise'\n",
            "highlight_color = 'limegreen'\n",
            "secondary_color = 'dodgerblue'\n",
            "highlight_model = 'deepseek-coder-6.7b-finetuned'\n",
            "secondary_model = 'gpt-4o-mini'\n",
            "\n",
            "colors = [main_color] * len(model_names)\n",
            "colors[model_names.index(highlight_model)] = highlight_color\n",
            "colors[model_names.index(secondary_model)] = secondary_color\n",
            "\n",
            "y_lim = min(max(max(accuracies), max(f1_scores)) + 0.1, 1)\n",
            "\n",
            "# Plot Accuracy\n",
            "bars_acc = ax[0].bar(model_names, accuracies, color=colors)\n",
            "ax[0].set_ylim([0, y_lim])\n",
            "ax[0].set_title('Accuracy', fontsize=16)\n",
            "ax[0].grid(axis='y')\n",
            "ax[0].set_xticks(np.arange(len(model_names)))\n",
            "ax[0].set_xticklabels(model_names, rotation=45, ha='right', fontsize=14)\n",
            "\n",
            "# Adding accuracy values above the bars\n",
            "for idx, bar in enumerate(bars_acc):\n",
            "    ax[0].text(bar.get_x() + bar.get_width() / 2.0, bar.get_height() + 0.01, f'{accuracies[idx]:.3f}', ha='center', va='bottom', fontsize=12, color='black')\n",
            "\n",
            "# Plot F1 Score\n",
            "bars_f1 = ax[1].bar(model_names, f1_scores, color=colors)\n",
            "ax[1].set_ylim([0, y_lim])\n",
            "ax[1].set_title('F1 Score (Binary)', fontsize=16)\n",
            "ax[1].grid(axis='y')\n",
            "ax[1].set_xticks(np.arange(len(model_names)))\n",
            "ax[1].set_xticklabels(model_names, rotation=45, ha='right', fontsize=14)\n",
            "\n",
            "# Adding F1 score values above the bars\n",
            "for idx, bar in enumerate(bars_f1):\n",
            "    ax[1].text(bar.get_x() + bar.get_width() / 2.0, bar.get_height() + 0.01, f'{f1_scores[idx]:.3f}', ha='center', va='bottom', fontsize=12, color='black')\n",
            "\n",
            "plt.tight_layout()\n",
            "plt.savefig('all-recommendations.pdf', format='pdf', bbox_inches='tight', pad_inches=0)\n",
            "plt.savefig('all-recommendations.png', format='png', bbox_inches='tight', pad_inches=0)\n",
            "plt.show()"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "rl",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.7"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
