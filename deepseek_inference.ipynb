{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from vllm import LLM, SamplingParams\n",
    "\n",
    "max_model_len, tp_size = 8192, 1\n",
    "model_name = \"deepseek-ai/DeepSeek-V2-Lite-Chat\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "llm = LLM(model=model_name, tensor_parallel_size=tp_size, max_model_len=max_model_len, trust_remote_code=True, enforce_eager=True)\n",
    "sampling_params = SamplingParams(temperature=0.3, max_tokens=256, stop_token_ids=[tokenizer.eos_token_id])\n",
    "\n",
    "messages_list = [\n",
    "    [{\"role\": \"user\", \"content\": \"Who are you?\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Translate the following content into Chinese directly: DeepSeek-V2 adopts innovative architectures to guarantee economical training and efficient inference.\"}],\n",
    "    [{\"role\": \"user\", \"content\": \"Write a piece of quicksort code in C++.\"}],\n",
    "]\n",
    "\n",
    "prompt_token_ids = [tokenizer.apply_chat_template(messages, add_generation_prompt=True) for messages in messages_list]\n",
    "\n",
    "outputs = llm.generate(prompt_token_ids=prompt_token_ids, sampling_params=sampling_params)\n",
    "\n",
    "generated_text = [output.outputs[0].text for output in outputs]\n",
    "print(generated_text)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
