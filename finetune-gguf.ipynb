{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "max_length = 1000\n",
    "# Load the model\n",
    "model_name = \"deepseek-ai/deepseek-coder-6.7b-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map='auto'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \\\n",
    "\"\"\"\n",
    "Below are one or more Solidity codeblocks. The codeblocks might contain vulnerable code.\n",
    "If there is a vulnerability please provide a description of the vulnearblity in terms of the code that is responsible for it.\n",
    "Describe how an attacker would be able to take advantage of the vulnerability so the explanation is even more clear.\n",
    "\n",
    "Output only the description of the vulnerability and the attacking vector. No additional information is needed.\n",
    "\n",
    "If there is no vulnerability output \"There is no vulnearbility\".\n",
    "\n",
    "Codeblocks:\n",
    "{}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\n",
    "    \"msc-smart-contract-auditing/audits-with-reasons\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unescape_newlines(cell):\n",
    "    if cell is None:\n",
    "        return None\n",
    "    return cell.replace(\"\\\\n\", \"\\n\")\n",
    "\n",
    "def prepare(row):\n",
    "    data = tokenizer(\n",
    "        text=prompt.format(row[\"code\"]),\n",
    "        text_target=row[\"description\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length\n",
    "    )\n",
    "\n",
    "    data['labels'] = data['input_ids'].copy()\n",
    "    return data\n",
    "\n",
    "train_dataset = dataset[\"train\"].to_pandas().map(unescape_newlines)\n",
    "test_dataset = dataset[\"test\"].to_pandas().map(unescape_newlines)\n",
    "\n",
    "train_prompts = train_dataset.apply(prepare, axis=1)\n",
    "test_prompts = test_dataset.apply(prepare, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "# Configuration for LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank\n",
    "    lora_alpha=32,  # scaling factor\n",
    "    # target_modules=[\"q_proj\", \"v_proj\"],  # Specify which modules to apply LoRA\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n",
    "    lora_dropout=0.05,  # dropout rate for LoRA layers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=TrainingArguments(\n",
    "        per_device_train_batch_size = 2,\n",
    "        gradient_accumulation_steps = 4,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 20,\n",
    "        learning_rate = 2e-4,\n",
    "        fp16 = not torch.cuda.is_bf16_supported(),\n",
    "        bf16 = torch.cuda.is_bf16_supported(),\n",
    "        logging_steps = 1,\n",
    "        optim = \"adamw_8bit\",\n",
    "        weight_decay = 0.01,\n",
    "        lr_scheduler_type = \"linear\",\n",
    "        seed = 3407,\n",
    "        output_dir = \"outputs\",\n",
    "    ),\n",
    "    train_dataset=train_prompts,\n",
    "    eval_dataset=test_prompts,\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"./finetuned_model\")\n",
    "tokenizer.save_pretrained(\"./finetuned_model\")\n",
    "\n",
    "# Evaluation\n",
    "results = trainer.evaluate()\n",
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
