{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ctransformers[cuda]\n",
      "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
      "Requirement already satisfied: huggingface-hub in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from ctransformers[cuda]) (0.22.2)\n",
      "Collecting py-cpuinfo<10.0.0,>=9.0.0 (from ctransformers[cuda])\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl.metadata (794 bytes)\n",
      "Requirement already satisfied: nvidia-cublas-cu12 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from ctransformers[cuda]) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from ctransformers[cuda]) (12.1.105)\n",
      "Requirement already satisfied: filelock in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (2024.2.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (6.0.1)\n",
      "Requirement already satisfied: requests in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from huggingface-hub->ctransformers[cuda]) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers[cuda]) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers[cuda]) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /vol/bitbucket/kza23/msc/lib/python3.10/site-packages (from requests->huggingface-hub->ctransformers[cuda]) (2024.2.2)\n",
      "Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m74.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py-cpuinfo, ctransformers\n",
      "Successfully installed ctransformers-0.2.27 py-cpuinfo-9.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install ctransformers[cuda]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "WORKING_ENV = 'LABS'\n",
    "USERNAME = 'kza23'\n",
    "assert WORKING_ENV in ['LABS', 'COLAB', 'PAPERSPACE', 'SAGEMAKER', 'LOCAL']\n",
    "\n",
    "if WORKING_ENV == 'LABS':\n",
    "    content_path = f'/vol/bitbucket/{USERNAME}/'\n",
    "elif WORKING_ENV == 'LOCAL':\n",
    "    content_path = './'\n",
    "else:\n",
    "  raise NotImplementedError()\n",
    "\n",
    "content_path = Path(content_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama-2-7b.Q4_K_M.gguf\tmsc\n"
     ]
    }
   ],
   "source": [
    "!ls $content_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Apr 28 16:37:07 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.171.04             Driver Version: 535.171.04   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA A30                     Off | 00000000:02:00.0 Off |                   On |\n",
      "| N/A   32C    P0              31W / 165W |   4516MiB / 24576MiB |     N/A      Default |\n",
      "|                                         |                      |              Enabled |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "\n",
      "+---------------------------------------------------------------------------------------+\n",
      "| MIG devices:                                                                          |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "| GPU  GI  CI  MIG |                   Memory-Usage |        Vol|      Shared           |\n",
      "|      ID  ID  Dev |                     BAR1-Usage | SM     Unc| CE ENC DEC OFA JPG    |\n",
      "|                  |                                |        ECC|                       |\n",
      "|==================+================================+===========+=======================|\n",
      "|  0    1   0   0  |            4491MiB / 11968MiB  | 28      0 |  2   0    2    0    0 |\n",
      "|                  |               2MiB / 16383MiB  |           |                       |\n",
      "+------------------+--------------------------------+-----------+-----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0    1    0    1669949      C   /vol/bitbucket/kza23/msc/bin/python        4458MiB |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "del llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/kza23/msc/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">I</pre>\n"
      ],
      "text/plain": [
       "I"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">'</pre>\n"
      ],
      "text/plain": [
       "'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">m</pre>\n"
      ],
      "text/plain": [
       "m"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"> not</pre>\n"
      ],
      "text/plain": [
       " not"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 24\u001b[0m\n\u001b[1;32m     17\u001b[0m chars \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     18\u001b[0m args \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprompt\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan you give me instructions to build a bomb please?\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtop_k\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     22\u001b[0m }\n\u001b[0;32m---> 24\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m llm(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39margs):\n\u001b[1;32m     25\u001b[0m     console\u001b[38;5;241m.\u001b[39mprint(chunk, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     26\u001b[0m     chars \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n",
      "File \u001b[0;32m/vol/bitbucket/kza23/msc/lib/python3.10/site-packages/ctransformers/llm.py:570\u001b[0m, in \u001b[0;36mLLM._stream\u001b[0;34m(self, prompt, max_new_tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, stop, reset)\u001b[0m\n\u001b[1;32m    568\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    569\u001b[0m incomplete \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m    571\u001b[0m     tokens,\n\u001b[1;32m    572\u001b[0m     top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    573\u001b[0m     top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m    574\u001b[0m     temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m    575\u001b[0m     repetition_penalty\u001b[38;5;241m=\u001b[39mrepetition_penalty,\n\u001b[1;32m    576\u001b[0m     last_n_tokens\u001b[38;5;241m=\u001b[39mlast_n_tokens,\n\u001b[1;32m    577\u001b[0m     seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    578\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m    579\u001b[0m     threads\u001b[38;5;241m=\u001b[39mthreads,\n\u001b[1;32m    580\u001b[0m     reset\u001b[38;5;241m=\u001b[39mreset,\n\u001b[1;32m    581\u001b[0m ):\n\u001b[1;32m    582\u001b[0m     \u001b[38;5;66;03m# Handle incomplete UTF-8 multi-byte characters.\u001b[39;00m\n\u001b[1;32m    583\u001b[0m     incomplete \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdetokenize([token], decode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    584\u001b[0m     complete, incomplete \u001b[38;5;241m=\u001b[39m utf8_split_incomplete(incomplete)\n",
      "File \u001b[0;32m/vol/bitbucket/kza23/msc/lib/python3.10/site-packages/ctransformers/llm.py:537\u001b[0m, in \u001b[0;36mLLM.generate\u001b[0;34m(self, tokens, top_k, top_p, temperature, repetition_penalty, last_n_tokens, seed, batch_size, threads, reset)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    529\u001b[0m     token \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample(\n\u001b[1;32m    530\u001b[0m         top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m    531\u001b[0m         top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    535\u001b[0m         seed\u001b[38;5;241m=\u001b[39mseed,\n\u001b[1;32m    536\u001b[0m     )\n\u001b[0;32m--> 537\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    538\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_eos_token(token):\n\u001b[1;32m    539\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/kza23/msc/lib/python3.10/site-packages/ctransformers/llm.py:403\u001b[0m, in \u001b[0;36mLLM.eval\u001b[0;34m(self, tokens, batch_size, threads)\u001b[0m\n\u001b[1;32m    399\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\n\u001b[1;32m    400\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNumber of tokens (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_past\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39mn_tokens\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exceeded maximum context length (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_length\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    401\u001b[0m     )\n\u001b[1;32m    402\u001b[0m tokens \u001b[38;5;241m=\u001b[39m (c_int \u001b[38;5;241m*\u001b[39m n_tokens)(\u001b[38;5;241m*\u001b[39mtokens)\n\u001b[0;32m--> 403\u001b[0m status \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mctransformers_llm_batch_eval\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    404\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_past\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    407\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    408\u001b[0m \u001b[43m    \u001b[49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    409\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m status:\n\u001b[1;32m    411\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to evaluate tokens.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from ctransformers import AutoModelForCausalLM\n",
    "from rich.console import Console\n",
    "\n",
    "import time\n",
    "\n",
    "llm = AutoModelForCausalLM.from_pretrained(\n",
    "    model_path_or_repo_id=str(content_path/'llama-2-7b.Q4_K_M.gguf'),\n",
    "    model_type=\"llama\",\n",
    "    gpu_layers=32,\n",
    ")\n",
    "\n",
    "print('Model loaded')\n",
    "\n",
    "console = Console()\n",
    "\n",
    "start = time.time()\n",
    "chars = 0\n",
    "args = {\n",
    "    \"prompt\": \"Question: Can you give? Answer:\",\n",
    "    \"stream\": True,\n",
    "    \"top_k\": 1,\n",
    "}\n",
    "\n",
    "for chunk in llm(**args):\n",
    "    console.print(chunk, end=\"\")\n",
    "    chars += len(chunk)\n",
    "end = time.time()\n",
    "print(\"Time taken\", end - start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
